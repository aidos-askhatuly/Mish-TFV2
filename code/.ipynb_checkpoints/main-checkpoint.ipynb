{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b199cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "import argparse\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfab9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from .nn_helpers import generator\n",
    "except:\n",
    "    from nn_helpers import generator\n",
    "try:\n",
    "    from .model import SemiHash\n",
    "except:\n",
    "    from model import SemiHash\n",
    "try:\n",
    "    from .eval_helpers import eval_hashing, acc_top_k_iterative, eval_min_hashing_iterative, get_labels_and_indices,\\\n",
    "        compute_hitrate_iterative, acc_top_k_tie_aware,threaded_acc_top_k_tie_aware, threaded_compute_hitrate_iterative\n",
    "except:\n",
    "    from eval_helpers import eval_hashing, acc_top_k_iterative, eval_min_hashing_iterative, get_labels_and_indices,\\\n",
    "        compute_hitrate_iterative, acc_top_k_tie_aware,threaded_acc_top_k_tie_aware, threaded_compute_hitrate_iterative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96698794",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from .run_over_log import run_iteration\n",
    "except:\n",
    "    from run_over_log import run_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90240c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67da9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['OPENBLAS_NUM_THREADS'] = '6'\n",
    "#os.environ['MKL_NUM_THREADS'] = '6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606983f9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_vectors_labels(dist_opposite, dist_same, top_k_prec, sess, handle, specific_handle, num_samples, batch_placeholder, is_training, sigma_anneal_vae,\n",
    "                           loss, emb_update, eval_batchsize, indices, labels, model, loss_doc_only, super_doc_only, rank_loss, nbr_loss, bit_balance_loss, prob_pair_loss,\n",
    "                           problem_pair, problem_pair_val, rotate_order, val_rotate_order, mem_updates=None):\n",
    "    total = num_samples\n",
    "    done = False\n",
    "    losses = []\n",
    "    losses_doc = []\n",
    "\n",
    "    sup_losses_doc = []\n",
    "    rank_losses_doc = []\n",
    "    nbr_losses_doc = []\n",
    "    top_k_prec_vals = []\n",
    "\n",
    "    bit_balance_loss_vals_l = []\n",
    "    prob_pair_loss_vals_l = []\n",
    "\n",
    "\n",
    "    dist_opposite_list = []\n",
    "    dist_same_list = []\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    while not done:\n",
    "        #print(total)\n",
    "        if mem_updates is None:\n",
    "            dist_opposite_val, dist_same_val, top_k_prec_val, lossvals, _, loss_doc_only_vals, sup_losses_doc_vals, rank_losses_doc_vals, nbr_losses_doc_vals, bit_balance_loss_vals, prob_pair_loss_vals =\\\n",
    "                dist_opposite, dist_same, top_k_prec, loss, emb_update, loss_doc_only, super_doc_only, rank_loss, nbr_loss, bit_balance_loss, prob_pair_loss\n",
    "        else:\n",
    "            top_k_prec_val, lossvals, _, loss_doc_only_vals, sup_losses_doc_vals, rank_losses_doc_vals, nbr_losses_doc_vals, bit_balance_loss_vals, _, _, _, _ = sess.run(\n",
    "                [top_k_prec, loss, emb_update, loss_doc_only, super_doc_only, rank_loss, nbr_loss, bit_balance_loss] + mem_updates[0],\n",
    "                feed_dict={handle: specific_handle, batch_placeholder: min(total, eval_batchsize),\n",
    "                           is_training: False, sigma_anneal_vae: 0})\n",
    "            mem_updates[1]()\n",
    "        losses += lossvals.tolist()\n",
    "        losses_doc += loss_doc_only_vals.tolist()\n",
    "\n",
    "        sup_losses_doc += sup_losses_doc_vals.tolist()\n",
    "        rank_losses_doc += rank_losses_doc_vals.tolist()\n",
    "        nbr_losses_doc += nbr_losses_doc_vals.tolist()\n",
    "        top_k_prec_vals += top_k_prec_val.tolist()\n",
    "\n",
    "        dist_opposite_list += dist_opposite_val.tolist()\n",
    "        dist_same_list += dist_same_val.tolist()\n",
    "\n",
    "        bit_balance_loss_vals_l.append(bit_balance_loss_vals)\n",
    "        prob_pair_loss_vals_l.append(prob_pair_loss_vals)\n",
    "\n",
    "        total -= len(lossvals)\n",
    "        if total <= 0:\n",
    "            done = True\n",
    "\n",
    "    #print(\"time\", time.time() - start)\n",
    "    losses = np.mean(losses)\n",
    "    losses_doc = np.mean(losses_doc)\n",
    "    sup_losses_doc = np.mean(sup_losses_doc)\n",
    "    rank_losses_doc = np.mean(rank_losses_doc)\n",
    "    nbr_losses_doc = np.mean(nbr_losses_doc)\n",
    "    top_k_prec_vals = np.mean(top_k_prec_vals)\n",
    "    dist_opposite_list = np.mean(dist_opposite_list)\n",
    "    dist_same_list = np.mean(dist_same_list)\n",
    "    bit_balance_loss_vals_l = np.mean(bit_balance_loss_vals_l)\n",
    "    prob_pair_loss_vals_l = np.mean(prob_pair_loss_vals_l)\n",
    "\n",
    "    embedding = model.get_hashcodes()\n",
    "\n",
    "    extracted_hashcodes = embedding[indices]\n",
    "    print(\"ones:\",np.sum(extracted_hashcodes)/np.sum(extracted_hashcodes>-2))\n",
    "    extracted_labels = [labels[i] for i in indices]\n",
    "\n",
    "    return losses, extracted_hashcodes, extracted_labels, losses_doc, sup_losses_doc, rank_losses_doc, nbr_losses_doc, top_k_prec_vals, dist_opposite_list, dist_same_list,bit_balance_loss_vals_l,prob_pair_loss_vals_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57070a4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def myprint(f,*x):\n",
    "    res = (\" \".join(str(y) for y in x))\n",
    "    print(res)\n",
    "    res = res + \"\\n\"\n",
    "    f.write(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d0d375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    \n",
    "    parser.add_argument(\"--block_size\", default=16, type=int)\n",
    "    parser.add_argument(\"--problem_pair\", default=1, type=float) # alpha_1\n",
    "    parser.add_argument(\"--new_top_k_pair\", default=-1, type=float) # alpha_2, sample one with high distance in top 100, that is hard to reach with multi index.\n",
    "\n",
    "    #scale topk with mem size vs train size\n",
    "    parser.add_argument(\"--scale_top_k\", default=1, type=int)\n",
    "    parser.add_argument(\"--new_top_k_pair_apply_at_block_distance\", default=2, type=int) #the distance in the block where the loss should try to contract around anchor. valid values either 1 or 2\n",
    "    \n",
    "    parser.add_argument(\"--memsize\", default=20000, type=int)\n",
    "    parser.add_argument(\"--numnbr\", default=10, type=int)\n",
    "\n",
    "    parser.add_argument(\"--batchsize\", default=40, type=int)\n",
    "    parser.add_argument(\"--bits\", default=32, type=int)\n",
    "    parser.add_argument(\"--lr\", default=0.001, type=float)\n",
    "    parser.add_argument(\"--dname\", default=\"reuters\", type=str)\n",
    "    parser.add_argument(\"--max_seq_size\", default=512, type=int)\n",
    "    parser.add_argument(\"--save_folder\", default=\"test_01_04_evening\", type=str)\n",
    "    parser.add_argument(\"--eval_every\", default=2000, type=int)\n",
    "    parser.add_argument(\"--maxiter\", default=30000, type=int)\n",
    "\n",
    "    parser.add_argument(\"--agg_loss\", default=1, type=int) #-1 = do nothing, 0 = mean_loss, 1 = sum_loss\n",
    "    parser.add_argument(\"--layersize\", default=1000, type=int)\n",
    "    parser.add_argument(\"--layers\", default=2, type=int)\n",
    "    parser.add_argument(\"--dropout\", default=0.8, type=float)\n",
    "    parser.add_argument(\"--doc2weight\", default=1.0, type=float) # nbr loss weight\n",
    "    parser.add_argument(\"--KLweight\", default=0.0, type=float)\n",
    "\n",
    "    #dont touch below\n",
    "    parser.add_argument(\"--anchor_problem_pair\", default=1, type=int)\n",
    "    parser.add_argument(\"--problem_pair_less_than\", default=1, type=int) #right now what we sample has to be exact distance, not it just need to be equal or less to have a problem\n",
    "\n",
    "    parser.add_argument(\"--type_bb\", default=\"pred_rev_block\", type=str) #pred_rev_block pred_one_block pred_one_all none\n",
    "    parser.add_argument(\"--coef_bb\", default=-1, type=float)\n",
    "    parser.add_argument(\"--problem_pair_choose_last\", default=1, type=int) #choose the pair with largetst distanc\n",
    "    parser.add_argument(\"--top_k_pair\", default=0, type=float) #tries to pull the top 100 more together, works as the scale of the loss\n",
    "    parser.add_argument(\"--top_k_pair_violate_extra_cost\", default=0, type=float) #should be kept above 1 if we want to primarily sample bad violators\n",
    "    parser.add_argument(\"--top_k_pair_hard_extra_cost\", default=0,type=float)  # should be either 0 or 1, depending on if we want to avoid having to do anything else than direct comparisons and punish this\n",
    "    parser.add_argument(\"--mask_equal_bits\", default=0, type=int) #only tries to push bits together that are not equal\n",
    "    parser.add_argument(\"--recon_pairs\", default=0, type=float) #weight of reconstruction of pair documents, normal doc is weigthed 1. Should never be above 1.\n",
    "\n",
    "    parser.add_argument(\"--sample_blocks\", default=1, type=int)\n",
    "\n",
    "    parser.add_argument(\"--problem_pair_batches\", default=0, type=int)\n",
    "\n",
    "    parser.add_argument(\"--rotate\", default=-1, type=int)\n",
    "    parser.add_argument(\"--threads_greedy\", default=4, type=int)\n",
    "    parser.add_argument(\"--down_sample_train\", default=8000, type=int)\n",
    "    parser.add_argument(\"--down_sample_val\", default=1000, type=int)\n",
    "\n",
    "    parser.add_argument(\"--use_importance_encoder\", default=-1, type=int)\n",
    "    parser.add_argument(\"--embedding_size\", default=-1, type=int) #if below 0, use bit size as embedding size, and dont use projection\n",
    "\n",
    "    parser.add_argument(\"--refresh_memory\", default=-1, type=int) #number of batches between a full update of memory module\n",
    "    parser.add_argument(\"--refresh_memory_batch_size\", default=1000, type=int)\n",
    "\n",
    "    parser.add_argument(\"--rankweight\", default=0.0, type=float)\n",
    "    parser.add_argument(\"--simoppositeweight\", default=0.0, type=float)\n",
    "    parser.add_argument(\"--superweight\", default=0.0, type=float)\n",
    "    parser.add_argument(\"--semisupervised\", default=0, type=int)\n",
    "    parser.add_argument(\"--datasettype\", default=\"1000\", type=str)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    print(args)\n",
    "    eval_batchsize = args.batchsize\n",
    "\n",
    "    os.makedirs(\"../results/\" + args.save_folder, exist_ok=True)\n",
    "\n",
    "    savename = \"../results/\" + args.save_folder + \"/\" + \"_\".join([str(v) for v in [args.dname, args.bits, args.batchsize, args.lr, args.KLweight, args.agg_loss,\n",
    "                                                       args.numnbr, args.eval_every, args.dropout, args.layers, args.layersize, args.maxiter, args.datasettype, args.semisupervised, args.memsize,\n",
    "                                                       args.superweight, args.rankweight]]) + \"_\" + str(np.random.randint(1000000000)) + str(np.random.randint(1000000000)) + str(np.random.randint(1000000000))\n",
    "\n",
    "    os.makedirs(savename)\n",
    "    args = vars(args)\n",
    "\n",
    "    basepath = lambda v1 : \"../data/datasets/\" + args[\"dname\"] + \"/\" + str(v1)\n",
    "\n",
    "    trainfiles = glob.glob(basepath(\"train_only*\"))\n",
    "    valfiles = glob.glob(basepath(\"val*\"))\n",
    "    testfiles = glob.glob(basepath(\"test*\"))\n",
    "    semitrainfiles = glob.glob(basepath(\"train_semi\" + \"_\" + str(args[\"datasettype\"] + \"*\")))\n",
    "\n",
    "    print([len(v) for v in [trainfiles, valfiles, testfiles, semitrainfiles]])\n",
    "    print(trainfiles)\n",
    "    labels, train_indices, val_indices, test_indices, data_text_vect, id2token, num_labels = get_labels_and_indices(args[\"dname\"])\n",
    "    args[\"num_labels\"] = num_labels\n",
    "    num_dataset_samples = len(labels)\n",
    "    bowlen = data_text_vect.shape[1]\n",
    "    args[\"bowlen\"] = bowlen\n",
    "    args[\"ndocs\"] = num_dataset_samples\n",
    "\n",
    "    print(\"----\", bowlen)\n",
    "\n",
    "    num_train_samples = sum([sum(1 for _ in tf.data.TFRecordDataset(file)) for file in trainfiles])\n",
    "    num_val_samples = sum([sum(1 for _ in tf.data.TFRecordDataset(file)) for file in valfiles])\n",
    "    num_test_samples = sum([sum(1 for _ in tf.data.TFRecordDataset(file)) for file in testfiles])\n",
    "    num_semitrain_samples = sum([sum(1 for _ in tf.data.TFRecordDataset(file)) for file in semitrainfiles])\n",
    "\n",
    "    print(num_train_samples, num_val_samples, num_test_samples)\n",
    "\n",
    "    print(\"num train\", num_train_samples)\n",
    "\n",
    "    if args[\"semisupervised\"]:\n",
    "        check_num_samples = num_semitrain_samples\n",
    "    else:\n",
    "        check_num_samples = num_train_samples\n",
    "    args[\"check_num_samples\"] = check_num_samples\n",
    "\n",
    "\n",
    "    if check_num_samples < args[\"memsize\"]:\n",
    "        args[\"memsize\"] = check_num_samples\n",
    "    print(\"!! memsize\", check_num_samples)\n",
    "\n",
    "    #tf.reset_default_graph()\n",
    "    \n",
    "    \n",
    "    handle = tf.Variable(\"\", dtype=tf.string, name=\"handle_iterator\")\n",
    "    training_handle, train_iter = generator(args[\"max_seq_size\"], handle, args[\"batchsize\"], trainfiles, 0, bowlen, num_labels)\n",
    "    #training_handle, train_iter, gen_iter = generator(args[\"max_seq_size\"], handle, args[\"batchsize\"], trainfiles, 0, bowlen, num_labels)\n",
    "    training_single_handle, training_single_iter = generator(args[\"max_seq_size\"], handle, eval_batchsize, trainfiles, 1, bowlen, num_labels)\n",
    "    val_handle, val_iter = generator(args[\"max_seq_size\"], handle, eval_batchsize, valfiles, 1, bowlen, num_labels)\n",
    "    test_handle, test_iter = generator(args[\"max_seq_size\"], handle, eval_batchsize, testfiles, 1, bowlen, num_labels)\n",
    "    semitrain_handle, semitrain_iter = generator(args[\"max_seq_size\"], handle, eval_batchsize, semitrainfiles, 1, bowlen, num_labels)\n",
    "        \n",
    "    if args['refresh_memory'] > 0:\n",
    "        training_refresh_handle, training_refresh_iter = generator(args[\"max_seq_size\"], handle,\n",
    "                                                                        args['refresh_memory_batch_size'], trainfiles, 0, bowlen,\n",
    "                                                                        num_labels)\n",
    "        training_refresh_iter.initializer()\n",
    "\n",
    "\n",
    "    sample = train_iter.get_next()\n",
    "\n",
    "        # batch_placeholder, is_training, sigma_anneal_vae, problem_pair, rotate_order are not needed as placeholders\n",
    "\t# Directly pass values to the model\n",
    "    batch_placeholder = args[\"batchsize\"]\n",
    "    is_training = True\n",
    "    sigma_anneal_vae = 0.0\n",
    "    problem_pair = args[\"problem_pair\"]\n",
    "    rotate_order = np.arange(0, args['bits'], dtype=np.int32)\n",
    "    val_rotate_order = np.arange(0, args['bits'], dtype=np.int32)\n",
    "\n",
    "\n",
    "\n",
    "    model = SemiHash(sample, args, batch_placeholder, is_training, sigma_anneal_vae, num_dataset_samples, problem_pair, rotate_order, num_train_samples)\n",
    "\n",
    "    train_op, loss, emb_update, loss_doc_only, mem_updates, update_mem_indices, \\\n",
    "    supervised_doc_loss, rank_loss, nbr_loss, dist_opposite, dist_same, top_k_prec, top_k_sims =\\\n",
    "            model.make_network(data_text_vect)\n",
    "\n",
    "    valid_pairs_problem_pair, valid_pairs_top_k_pair = model.return_valid_pairs()\n",
    "\n",
    "    if args['refresh_memory'] > 0:\n",
    "            refresh_mem_updates, refresh_update_mem_indices = model.refresh_memory_module(sample, args['refresh_memory_batch_size'])\n",
    "\n",
    "    loss_bit_balance = model.return_bit_balance_loss()\n",
    "    prob_pair_loss,_ = model.return_prob_pair_loss()\n",
    "\n",
    "    running = True\n",
    "    losses = []\n",
    "    train_count = 0\n",
    "    buffer_count = 0\n",
    "    vae_val = 1.0\n",
    "    vae_val_reduction = 1e-6\n",
    "\n",
    "    patience_max = 5\n",
    "    patience_current = 0\n",
    "    best_val_loss = 100000000\n",
    "\n",
    "    best_embeddings_list = []\n",
    "\n",
    "    test_perf = []\n",
    "    val_perf = []\n",
    "    times = []\n",
    "    top_k_prec_list = []\n",
    "    val_losses_list = []\n",
    "    val_losses_doc_list = []\n",
    "\n",
    "    list_val_opposite = []\n",
    "    list_val_same = []\n",
    "\n",
    "    list_val_iter_perf = []\n",
    "    list_test_iter_perf = []\n",
    "\n",
    "    list_val_hitrate = []\n",
    "    list_test_hitrate = []\n",
    "\n",
    "    list_val_violations = []\n",
    "    list_test_violations = []\n",
    "\n",
    "    list_val_avg_topK = []\n",
    "    list_test_avg_topK = []\n",
    "\n",
    "    valid_pairs_problem_pair_l = []\n",
    "    valid_pairs_top_k_pair_l = []\n",
    "\n",
    "    time_before_problem_loss = args[\"problem_pair_batches\"]\n",
    "    if time_before_problem_loss == 0:\n",
    "        problem_pair_val = args[\"problem_pair\"]\n",
    "    else:\n",
    "        problem_pair_val = 0\n",
    "\n",
    "    with open(savename + \"/run_log\", \"a\") as file_handle:\n",
    "        while running:\n",
    "            start_time = time.time()\n",
    "\n",
    "                # fill up memory buffers first, run for 1 epoch\n",
    "            if buffer_count*args[\"batchsize\"]/check_num_samples < 1.0:\n",
    "                for update in mem_updates:\n",
    "                    update  # Executes the TensorFlow update operation\n",
    "                for update in update_mem_indices:\n",
    "                    update  # Executes the TensorFlow update operation\n",
    "                buffer_count += 1\n",
    "                continue\n",
    "\n",
    "                val_top_k_sims, val_top_k_prec, val_opposite, val_same, lossval = model_computation_fn(training_handle, args[\"batchsize\"], True, vae_val, problem_pair_val, val_rotate_order)\n",
    "                \n",
    "                update_mem_indices()\n",
    "\n",
    "                losses+= lossval.tolist()\n",
    "                train_count += 1\n",
    "                vae_val = max(vae_val - vae_val_reduction, 0)\n",
    "                top_k_prec_list += val_top_k_prec.tolist()\n",
    "\n",
    "                list_val_opposite.append(np.mean(val_opposite))\n",
    "                list_val_same.append(np.mean(val_same))\n",
    "\n",
    "                times.append(time.time() - start_time)\n",
    "\n",
    "                valid_pairs_problem_pair_l.append(valid_pairs_problem_pair_r)\n",
    "                valid_pairs_top_k_pair_l.append(valid_pairs_top_k_pair_r)\n",
    "\n",
    "                time_before_problem_loss-=1\n",
    "                if time_before_problem_loss == 0:\n",
    "                    problem_pair_val = args['problem_pair']\n",
    "                    print(\"UPDATED PROBLEM PAIR COEF TO BE USED !!\")\n",
    "\n",
    "                #update memory with fresh codes\n",
    "                if args['refresh_memory'] > 0 and train_count > 0 and train_count % args['refresh_memory'] == 0:\n",
    "                    num_refresh_updates = int(np.ceil(args['memsize']/ args['refresh_memory_batch_size']))\n",
    "                    \n",
    "                    for i in range(num_refresh_updates):\n",
    "                        refresh_mem_updates(training_refresh_handle, is_training - False)\n",
    "                        refresh_update_mem_indices()\n",
    "\n",
    "                if train_count > 0 and train_count % args[\"eval_every\"] == 0:\n",
    "                    myprint(file_handle,\"patience\", patience_current)\n",
    "                    myprint(file_handle,\"Training\", np.mean(losses), \"vae_val\", vae_val, \"epochs\", train_count*args[\"batchsize\"]/num_train_samples, \"top_k_prec\", np.mean(top_k_prec_list))\n",
    "                    myprint(file_handle,\"% of valid pairs\", np.mean(valid_pairs_problem_pair_l), np.mean(valid_pairs_top_k_pair_l))\n",
    "                    myprint(file_handle,\"mean train time\", np.mean(times), \"opposite\", np.mean(list_val_opposite), \"same\", np.mean(list_val_same))\n",
    "\n",
    "\n",
    "                    times = []\n",
    "                    top_k_prec_list = []\n",
    "                    losses = losses[-(num_train_samples):]\n",
    "\n",
    "                    problem_pair_dist_0_val_l = []\n",
    "\n",
    "                    valid_pairs_problem_pair_l = []\n",
    "                    valid_pairs_top_k_pair_l = []\n",
    "\t\t    \n",
    "\t\t    # Initialize the iterators\n",
    "\t\t    \n",
    "                    train_iter.initializer()\n",
    "                    training_single_iter.initializer()\n",
    "                    val_iter.initializer()\n",
    "                    test_iter.initializer()\n",
    "                    semitrain_iter.initializer()\n",
    "                    \n",
    "                    trainloss, train_hashcodes, train_labels, train_losses_doc, \\\n",
    "                    train_sup_losses_doc, train_rank_losses_doc, train_nbr_losses_doc, topkprec_train, \\\n",
    "                    dist_opposite_list_train, dist_same_list_train, train_bit_balance_loss, train_prob_pair_loss =\\\n",
    "                        extract_vectors_labels(dist_opposite, dist_same, top_k_prec, handle, training_single_handle,\n",
    "                                                                                      num_train_samples,\n",
    "                                                                                      batch_placeholder, is_training,\n",
    "                                                                                      sigma_anneal_vae, loss, emb_update,\n",
    "                                                                                      eval_batchsize, train_indices, labels,\n",
    "                                                                                      model, loss_doc_only, supervised_doc_loss, rank_loss, nbr_loss, loss_bit_balance,\n",
    "                                                                                                prob_pair_loss,problem_pair,problem_pair_val,\n",
    "                                                                                        rotate_order, val_rotate_order, mem_updates=None)\n",
    "\n",
    "                    valloss, val_hashcodes, val_labels, val_losses_doc, \\\n",
    "                    val_sup_losses_doc, val_rank_losses_doc, val_nbr_losses_doc, topkprec_val, \\\n",
    "                    dist_opposite_list_val, dist_same_list_val, val_bit_balance_loss, val_prob_pair_loss =\\\n",
    "                        extract_vectors_labels(dist_opposite, dist_same, top_k_prec, handle, val_handle,\n",
    "                                                                                      num_val_samples,\n",
    "                                                                                      batch_placeholder, is_training,\n",
    "                                                                                      sigma_anneal_vae, loss, emb_update,\n",
    "                                                                                      eval_batchsize, val_indices, labels,\n",
    "                                                                                      model, loss_doc_only, supervised_doc_loss, rank_loss, nbr_loss, loss_bit_balance,\n",
    "                                                                                              prob_pair_loss,problem_pair,problem_pair_val,\n",
    "                                                                                        rotate_order, val_rotate_order, mem_updates=None)\n",
    "                                                                                        \n",
    "\n",
    "                    testloss, test_hashcodes, test_labels, test_losses_doc, \\\n",
    "                    test_sup_losses_doc, test_rank_losses_doc, test_nbr_losses_doc, topkprec_test, \\\n",
    "                    dist_opposite_list_test, dist_same_list_test, test_bit_balance_loss, test_prob_pair_loss =\\\n",
    "                        extract_vectors_labels(dist_opposite, dist_same, top_k_prec, handle, test_handle,\n",
    "                                                                                      num_test_samples,\n",
    "                                                                                      batch_placeholder, is_training,\n",
    "                                                                                      sigma_anneal_vae, loss, emb_update,\n",
    "                                                                                      eval_batchsize, test_indices, labels,\n",
    "                                                                                      model, loss_doc_only, supervised_doc_loss, rank_loss, nbr_loss, loss_bit_balance,\n",
    "                                                                                             prob_pair_loss,problem_pair,problem_pair_val,\n",
    "                                                                                       rotate_order, val_rotate_order,mem_updates=None)\n",
    "\n",
    "\n",
    "                    if args['rotate'] > 0:\n",
    "                        print(\"START FINDING NEW ORDER\")\n",
    "                        time_start = time.time()\n",
    "                        perm_train = np.random.permutation(len(train_hashcodes))[:args['down_sample_train']]\n",
    "                        train_hashcodes = train_hashcodes[perm_train]\n",
    "                        perm_val = np.random.permutation(len(val_hashcodes))[:args['down_sample_val']]\n",
    "                        val_hashcodes = val_hashcodes[perm_val]\n",
    "\n",
    "                        val_rotate_order, _, _ = run_iteration(train_hashcodes, val_hashcodes, None, bits=args['bits'],\n",
    "                                                         n_blocks=int(args['bits']/args['block_size']),\n",
    "                                                         threads=args['threads_greedy'], report_test=False,\n",
    "                                                         rotate_order=val_rotate_order, max_switch=1)\n",
    "\n",
    "                        print(\"new order: \", val_rotate_order, \"time taken: \", time.time() - time_start)\n",
    "                                  \n",
    "                        train_iter.initialize()\n",
    "                        training_single_iter.initialize()\n",
    "                        val_iter.initialize()\n",
    "                        test_iter.initialize()\n",
    "                        semitrain_iter.initialize()\n",
    "\n",
    "                        trainloss, train_hashcodes, train_labels, train_losses_doc, \\\n",
    "                        train_sup_losses_doc, train_rank_losses_doc, train_nbr_losses_doc, topkprec_train, \\\n",
    "                        dist_opposite_list_train, dist_same_list_train, train_bit_balance_loss, train_prob_pair_loss = \\\n",
    "                            extract_vectors_labels(dist_opposite, dist_same, top_k_prec, handle,\n",
    "                                                   training_single_handle,\n",
    "                                                   num_train_samples,\n",
    "                                                   batch_placeholder, is_training,\n",
    "                                                   sigma_anneal_vae, loss, emb_update,\n",
    "                                                   eval_batchsize, train_indices, labels,\n",
    "                                                   model, loss_doc_only, supervised_doc_loss, rank_loss, nbr_loss,\n",
    "                                                   loss_bit_balance,\n",
    "                                                   prob_pair_loss, problem_pair, problem_pair_val,\n",
    "                                                   rotate_order, val_rotate_order, mem_updates=None)\n",
    "\n",
    "                        valloss, val_hashcodes, val_labels, val_losses_doc, \\\n",
    "                        val_sup_losses_doc, val_rank_losses_doc, val_nbr_losses_doc, topkprec_val, \\\n",
    "                        dist_opposite_list_val, dist_same_list_val, val_bit_balance_loss, val_prob_pair_loss = \\\n",
    "                            extract_vectors_labels(dist_opposite, dist_same, top_k_prec, handle, val_handle,\n",
    "                                                   num_val_samples,\n",
    "                                                   batch_placeholder, is_training,\n",
    "                                                   sigma_anneal_vae, loss, emb_update,\n",
    "                                                   eval_batchsize, val_indices, labels,\n",
    "                                                   model, loss_doc_only, supervised_doc_loss, rank_loss, nbr_loss,\n",
    "                                                   loss_bit_balance,\n",
    "                                                   prob_pair_loss, problem_pair, problem_pair_val,\n",
    "                                                   rotate_order, val_rotate_order, mem_updates=None)\n",
    "\n",
    "                        testloss, test_hashcodes, test_labels, test_losses_doc, \\\n",
    "                        test_sup_losses_doc, test_rank_losses_doc, test_nbr_losses_doc, topkprec_test, \\\n",
    "                        dist_opposite_list_test, dist_same_list_test, test_bit_balance_loss, test_prob_pair_loss = \\\n",
    "                            extract_vectors_labels(dist_opposite, dist_same, top_k_prec, handle, test_handle,\n",
    "                                                   num_test_samples,\n",
    "                                                   batch_placeholder, is_training,\n",
    "                                                   sigma_anneal_vae, loss, emb_update,\n",
    "                                                   eval_batchsize, test_indices, labels,\n",
    "                                                   model, loss_doc_only, supervised_doc_loss, rank_loss, nbr_loss,\n",
    "                                                   loss_bit_balance,\n",
    "                                                   prob_pair_loss, problem_pair, problem_pair_val,\n",
    "                                                   rotate_order, val_rotate_order, mem_updates=None)\n",
    "\n",
    "\n",
    "                    val_prec100, valdists = eval_hashing(train_hashcodes, train_labels, val_hashcodes, val_labels)\n",
    "                    test_prec100, testdists = eval_hashing(train_hashcodes, train_labels, test_hashcodes, test_labels)\n",
    "\n",
    "                    #minwise hitrate\n",
    "                    t = time.time()\n",
    "                    '''\n",
    "                    val_hitrate, val_violations, val_avg_topK = threaded_compute_hitrate_iterative(train_hashcodes,val_hashcodes,args['bits'],int(args['bits']/args['block_size']))\n",
    "                    test_hitrate, test_violations, test_avg_topK = threaded_compute_hitrate_iterative(train_hashcodes,test_hashcodes,args['bits'],int(args['bits']/args['block_size']))\n",
    "                    '''\n",
    "                    val_hitrate, val_violations, val_avg_topK = compute_hitrate_iterative(train_hashcodes,val_hashcodes,args['bits'],int(args['bits']/args['block_size']), skip_hitrate_comp=True)\n",
    "                    test_hitrate, test_violations, test_avg_topK = compute_hitrate_iterative(train_hashcodes,test_hashcodes,args['bits'],int(args['bits']/args['block_size']), skip_hitrate_comp=True)\n",
    "                    print(\"time for hitrate:\", time.time()-t)\n",
    "\n",
    "                    #iterative prec100, that resolve ties by averaging the performance\n",
    "\n",
    "                    t=time.time()\n",
    "                    #val_prec100_iter, _, _, _ = acc_top_k_iterative(train_hashcodes, train_labels, val_hashcodes, val_labels, [100], args[\"num_labels\"])\n",
    "                    val_prec100_iter, _ = threaded_acc_top_k_tie_aware(train_hashcodes, train_labels, val_hashcodes, val_labels, [100], args[\"num_labels\"])\n",
    "                    val_prec100_iter = np.mean(val_prec100_iter)\n",
    "\n",
    "                    #test_prec100_iter, _, _, _ = acc_top_k_iterative(train_hashcodes, train_labels, test_hashcodes, test_labels, [100], args[\"num_labels\"])\n",
    "                    test_prec100_iter, _ = threaded_acc_top_k_tie_aware(train_hashcodes, train_labels, test_hashcodes,test_labels, [100], args[\"num_labels\"])\n",
    "                    test_prec100_iter = np.mean(test_prec100_iter)\n",
    "                    print(\"time for top_k:\", time.time()-t)\n",
    "\n",
    "\n",
    "                    myprint(file_handle,\"Train\", trainloss, train_losses_doc, train_sup_losses_doc, train_rank_losses_doc, train_nbr_losses_doc, train_bit_balance_loss, train_prob_pair_loss)\n",
    "                    myprint(file_handle,\"Val\", valdists, valloss, np.mean(val_prec100), val_losses_doc, val_sup_losses_doc, val_rank_losses_doc, val_nbr_losses_doc, val_bit_balance_loss, val_prob_pair_loss)\n",
    "                    myprint(file_handle,\"Testing\", testdists, testloss, np.mean(test_prec100), test_losses_doc, test_sup_losses_doc, test_rank_losses_doc, test_nbr_losses_doc, test_bit_balance_loss, test_prob_pair_loss)\n",
    "                    myprint(file_handle,\"topkprec_network\", topkprec_train, topkprec_val, topkprec_test, \"top iter 100\", val_prec100_iter, test_prec100_iter)\n",
    "                    myprint(file_handle,\"dist_opposites\",  dist_opposite_list_train, dist_opposite_list_val, dist_opposite_list_test)\n",
    "                    myprint(file_handle,\"dist_same\",  dist_same_list_train, dist_same_list_val, dist_same_list_test)\n",
    "                    myprint(file_handle,\"hit rate\", val_hitrate, test_hitrate)\n",
    "                    myprint(file_handle, \"violations\", val_violations, test_violations, \"avg tokK\", val_avg_topK, test_avg_topK)\n",
    "\n",
    "\n",
    "                    if best_val_loss > val_losses_doc:\n",
    "                        emb_matrix = model.get_hashcodes().numpy()\n",
    "                        best_embeddings = emb_matrix\n",
    "                        \n",
    "                        best_embeddings_list.append(best_embeddings)\n",
    "                        best_val_loss = val_losses_doc\n",
    "                        patience_current = 0\n",
    "                        test_perf.append(test_prec100)\n",
    "                        val_perf.append(np.mean(val_prec100))\n",
    "\n",
    "                        val_losses_list.append(valloss)\n",
    "                        val_losses_doc_list.append(val_losses_doc)\n",
    "\n",
    "                        list_val_iter_perf.append(val_prec100_iter)\n",
    "                        list_test_iter_perf.append(test_prec100_iter)\n",
    "\n",
    "                        list_val_hitrate.append(val_hitrate)\n",
    "                        list_test_hitrate.append(test_hitrate)\n",
    "\n",
    "                        list_val_violations.append(val_violations)\n",
    "                        list_test_violations.append(test_violations)\n",
    "\n",
    "\n",
    "                        list_val_avg_topK.append(val_avg_topK)\n",
    "                        list_test_avg_topK.append(test_avg_topK)\n",
    "\n",
    "\n",
    "\n",
    "                        pickle.dump([best_embeddings_list, args, best_val_loss, train_count, vae_val, test_perf, val_perf,\n",
    "                                     val_losses_list, val_losses_doc_list,\n",
    "                                     [dist_opposite_list_train,dist_same_list_train], [dist_opposite_list_val,dist_same_list_val], [dist_opposite_list_test,dist_same_list_test],\n",
    "                                     list_val_iter_perf, list_test_iter_perf, list_val_hitrate, list_test_hitrate, list_val_violations, list_test_violations,\n",
    "                                     list_val_avg_topK, list_test_avg_topK],\n",
    "                                    open(savename + \"/res.pkl\", \"wb\"))\n",
    "                    else:\n",
    "                        patience_current += 1\n",
    "\n",
    "                    if patience_current >= patience_max or train_count > args[\"maxiter\"]: #or ((time.time() - start_time) > (60*60*args[\"hours\"])):\n",
    "                        running = False\n",
    "                file_handle.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e38bb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfceb5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
